{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VernacularNameCategory</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>DepthInMeters</th>\n",
       "      <th>Oxygen</th>\n",
       "      <th>Salinity</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Phosphate</th>\n",
       "      <th>LocationAccuracy</th>\n",
       "      <th>Station_-</th>\n",
       "      <th>...</th>\n",
       "      <th>SamplingEquipment_submersible</th>\n",
       "      <th>SamplingEquipment_towed camera</th>\n",
       "      <th>SamplingEquipment_trawl</th>\n",
       "      <th>RecordType_NaN</th>\n",
       "      <th>RecordType_catch record</th>\n",
       "      <th>RecordType_literature</th>\n",
       "      <th>RecordType_notation</th>\n",
       "      <th>RecordType_specimen</th>\n",
       "      <th>RecordType_still image</th>\n",
       "      <th>RecordType_video observation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>36.87485</td>\n",
       "      <td>-121.92476</td>\n",
       "      <td>48.0</td>\n",
       "      <td>5.605830</td>\n",
       "      <td>33.207511</td>\n",
       "      <td>11.375480</td>\n",
       "      <td>0.874281</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>25.88676</td>\n",
       "      <td>-167.78026</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>1.469286</td>\n",
       "      <td>34.564939</td>\n",
       "      <td>2.561229</td>\n",
       "      <td>3.166553</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>37.36785</td>\n",
       "      <td>-123.39338</td>\n",
       "      <td>1294.0</td>\n",
       "      <td>0.896997</td>\n",
       "      <td>34.496119</td>\n",
       "      <td>3.085411</td>\n",
       "      <td>3.205367</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>51.50220</td>\n",
       "      <td>-177.92657</td>\n",
       "      <td>1078.0</td>\n",
       "      <td>0.506452</td>\n",
       "      <td>34.402537</td>\n",
       "      <td>3.266718</td>\n",
       "      <td>3.181331</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>29.06941</td>\n",
       "      <td>-88.37699</td>\n",
       "      <td>404.0</td>\n",
       "      <td>0.902513</td>\n",
       "      <td>34.207603</td>\n",
       "      <td>7.068033</td>\n",
       "      <td>2.990354</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1191 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   VernacularNameCategory  latitude  longitude  DepthInMeters    Oxygen  \\\n",
       "0                       2  36.87485 -121.92476           48.0  5.605830   \n",
       "1                       0  25.88676 -167.78026         2000.0  1.469286   \n",
       "2                       0  37.36785 -123.39338         1294.0  0.896997   \n",
       "3                       0  51.50220 -177.92657         1078.0  0.506452   \n",
       "4                       7  29.06941  -88.37699          404.0  0.902513   \n",
       "\n",
       "    Salinity  Temperature  Phosphate  LocationAccuracy  Station_-  \\\n",
       "0  33.207511    11.375480   0.874281             100.0          0   \n",
       "1  34.564939     2.561229   3.166553              50.0          0   \n",
       "2  34.496119     3.085411   3.205367             100.0          0   \n",
       "3  34.402537     3.266718   3.181331              20.0          0   \n",
       "4  34.207603     7.068033   2.990354              50.0          0   \n",
       "\n",
       "               ...               SamplingEquipment_submersible  \\\n",
       "0              ...                                           0   \n",
       "1              ...                                           0   \n",
       "2              ...                                           0   \n",
       "3              ...                                           0   \n",
       "4              ...                                           0   \n",
       "\n",
       "   SamplingEquipment_towed camera  SamplingEquipment_trawl  RecordType_NaN  \\\n",
       "0                               0                        0               0   \n",
       "1                               0                        0               0   \n",
       "2                               0                        0               0   \n",
       "3                               0                        0               0   \n",
       "4                               0                        0               1   \n",
       "\n",
       "   RecordType_catch record  RecordType_literature  RecordType_notation  \\\n",
       "0                        0                      0                    0   \n",
       "1                        0                      0                    0   \n",
       "2                        0                      0                    0   \n",
       "3                        0                      0                    0   \n",
       "4                        0                      0                    0   \n",
       "\n",
       "   RecordType_specimen  RecordType_still image  RecordType_video observation  \n",
       "0                    0                       0                             1  \n",
       "1                    0                       0                             1  \n",
       "2                    0                       0                             1  \n",
       "3                    0                       0                             1  \n",
       "4                    0                       0                             0  \n",
       "\n",
       "[5 rows x 1191 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"/Users/ellise/Desktop/untitled folder 2/df00.csv\")\n",
    "df.drop(\"Unnamed: 0\",axis = 1).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Train data shape: ', (6183, 1191))\n",
      "('Train labels shape: ', (6183,))\n",
      "('Test data shape: ', (1092, 1191))\n",
      "('Test labels shape: ', (1092,))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = np.array(df.drop(['VernacularNameCategory'],axis=1))\n",
    "Y = np.array(df[\"VernacularNameCategory\"])\n",
    "Y = Y.astype(int)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.15, random_state=42)\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', Y_train.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "a = torch.from_numpy(X_test)\n",
    "b = torch.from_numpy(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.FloatTensor\n",
    "a = a.type(dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "all_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-2.3244e+01 -2.2885e+01 -3.9571e+01  ...  -4.2763e+01 -4.1087e+01 -1.4885e+01\n",
       "-1.5521e+01 -1.5266e+01 -2.6386e+01  ...  -2.8545e+01 -2.7408e+01 -9.9174e+00\n",
       "-7.3062e+00 -7.1641e+00 -1.2365e+01  ...  -1.3410e+01 -1.2842e+01 -4.6197e+00\n",
       "                ...                   ⋱                   ...                \n",
       "-1.3281e+01 -1.3421e+01 -2.2693e+01  ...  -2.4627e+01 -2.3763e+01 -8.4799e+00\n",
       "-2.9935e+00 -2.9689e+00 -4.8045e+00  ...  -5.2563e+00 -5.0302e+00 -1.9564e+00\n",
       "-7.3066e+00 -7.2854e+00 -1.2359e+01  ...  -1.3498e+01 -1.2986e+01 -4.6697e+00\n",
       "[torch.FloatTensor of size 1092x10]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 1132.772705078125)\n",
      "(1, 1132.39111328125)\n",
      "(2, 32.291748046875)\n",
      "(3, 1131.0184326171875)\n",
      "(4, 120.41736602783203)\n",
      "(5, 241.85568237304688)\n",
      "(6, 241.7765350341797)\n",
      "(7, 2272.981689453125)\n",
      "(8, 2271.31884765625)\n",
      "(9, 32.27882766723633)\n",
      "(10, 2265.51171875)\n",
      "(11, 284.77850341796875)\n",
      "(12, 241.1358184814453)\n",
      "(13, 32.271976470947266)\n",
      "(14, 284.69732666015625)\n",
      "(15, 240.73695373535156)\n",
      "(16, 120.32613372802734)\n",
      "(17, 1122.2784423828125)\n",
      "(18, 32.250492095947266)\n",
      "(19, 32.2459831237793)\n",
      "(20, 120.28337860107422)\n",
      "(21, 120.27188110351562)\n",
      "(22, 1119.438720703125)\n",
      "(23, 284.4140319824219)\n",
      "(24, 120.23226928710938)\n",
      "(25, 1117.2906494140625)\n",
      "(26, 32.21677780151367)\n",
      "(27, 284.2856140136719)\n",
      "(28, 284.2456970214844)\n",
      "(29, 284.1948547363281)\n",
      "(30, 2228.987060546875)\n",
      "(31, 239.0393524169922)\n",
      "(32, 120.13055419921875)\n",
      "(33, 283.9734191894531)\n",
      "(34, 238.6962127685547)\n",
      "(35, 120.0915756225586)\n",
      "(36, 2219.00244140625)\n",
      "(37, 1109.9468994140625)\n",
      "(38, 1109.2005615234375)\n",
      "(39, 283.6619873046875)\n",
      "(40, 32.15470886230469)\n",
      "(41, 283.5602111816406)\n",
      "(42, 32.14657211303711)\n",
      "(43, 1104.8955078125)\n",
      "(44, 283.4002380371094)\n",
      "(45, 283.3419494628906)\n",
      "(46, 2200.55029296875)\n",
      "(47, 32.1268196105957)\n",
      "(48, 237.251220703125)\n",
      "(49, 119.92827606201172)\n",
      "(50, 32.11442565917969)\n",
      "(51, 1098.9183349609375)\n",
      "(52, 236.80813598632812)\n",
      "(53, 282.9158020019531)\n",
      "(54, 119.86927032470703)\n",
      "(55, 2184.458740234375)\n",
      "(56, 282.7664794921875)\n",
      "(57, 236.13546752929688)\n",
      "(58, 32.07525634765625)\n",
      "(59, 1093.812744140625)\n",
      "(60, 32.064697265625)\n",
      "(61, 282.5155334472656)\n",
      "(62, 1091.663818359375)\n",
      "(63, 119.763671875)\n",
      "(64, 119.7504653930664)\n",
      "(65, 2167.07080078125)\n",
      "(66, 282.2897033691406)\n",
      "(67, 119.71224212646484)\n",
      "(68, 282.19561767578125)\n",
      "(69, 1086.43115234375)\n",
      "(70, 282.08966064453125)\n",
      "(71, 234.6369171142578)\n",
      "(72, 1084.03466796875)\n",
      "(73, 281.9159240722656)\n",
      "(74, 234.2700653076172)\n",
      "(75, 119.60443115234375)\n",
      "(76, 233.93971252441406)\n",
      "(77, 119.57199096679688)\n",
      "(78, 31.97989273071289)\n",
      "(79, 119.5381851196289)\n",
      "(80, 281.53668212890625)\n",
      "(81, 281.4836730957031)\n",
      "(82, 119.4896011352539)\n",
      "(83, 119.4730224609375)\n",
      "(84, 119.45560455322266)\n",
      "(85, 119.4374008178711)\n",
      "(86, 1075.9998779296875)\n",
      "(87, 119.39938354492188)\n",
      "(88, 232.27964782714844)\n",
      "(89, 31.916440963745117)\n",
      "(90, 231.9980010986328)\n",
      "(91, 119.32233428955078)\n",
      "(92, 281.0019836425781)\n",
      "(93, 2141.611328125)\n",
      "(94, 31.885366439819336)\n",
      "(95, 280.8774108886719)\n",
      "(96, 31.875150680541992)\n",
      "(97, 2135.67724609375)\n",
      "(98, 230.8648681640625)\n",
      "(99, 230.7064971923828)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import random\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class DynamicNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        In the constructor we construct three nn.Linear instances that we will use\n",
    "        in the forward pass.\n",
    "        \"\"\"\n",
    "        super(DynamicNet, self).__init__()\n",
    "        self.input_linear = torch.nn.Linear(D_in, H)\n",
    "        self.middle_linear = torch.nn.Linear(H, H)\n",
    "        self.output_linear = torch.nn.Linear(H, D_out)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        For the forward pass of the model, we randomly choose either 0, 1, 2, or 3\n",
    "        and reuse the middle_linear Module that many times to compute hidden layer\n",
    "        representations.\n",
    "\n",
    "        Since each forward pass builds a dynamic computation graph, we can use normal\n",
    "        Python control-flow operators like loops or conditional statements when\n",
    "        defining the forward pass of the model.\n",
    "\n",
    "        Here we also see that it is perfectly safe to reuse the same Module many\n",
    "        times when defining a computational graph. This is a big improvement from Lua\n",
    "        Torch, where each Module could be used only once.\n",
    "        \"\"\"\n",
    "        h_relu = self.input_linear(x).clamp(min=0)#Clamps all elements into the range [min_value, max_value](ReLU here)\n",
    "        for _ in range(random.randint(0, 5)):\n",
    "            h_relu = self.middle_linear(h_relu).clamp(min=0)\n",
    "        y_pred = self.output_linear(h_relu)\n",
    "        y_pred = self.softmax(y_pred)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 6183, 1191, 400, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs, and wrap them in Variables\n",
    "x = Variable(a)\n",
    "y = Variable(b, requires_grad=False)\n",
    "\n",
    "# Construct our model by instantiating the class defined above\n",
    "model = DynamicNet(D_in, H, D_out)\n",
    "\n",
    "# Construct our loss function and an Optimizer. Training this strange model with\n",
    "# vanilla stochastic gradient descent is tough, so we use momentum\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-10, momentum=0.9)\n",
    "for t in range(100):\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y)\n",
    "    print(t, loss.data[0])\n",
    "    all_losses.append(loss.data[0])\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topk_n, topk_i = model(x).data.topk(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i in xrange(Y.size):\n",
    "    if topk_i[i][0] == Y[i]:\n",
    "        count+=1\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topk_i[3][0] != Y[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "x = Variable(torch.ones(2, 2), requires_grad = True)\n",
    "y = x + 2\n",
    "y.grad_fn\n",
    "\n",
    "# y 是作为一个操作的结果创建的因此y有一个creator \n",
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "\n",
    "# 现在我们来使用反向传播\n",
    "out.backward()\n",
    "\n",
    "# out.backward()和操作out.backward(torch.Tensor([1.0]))是等价的\n",
    "# 在此处输出 d(out)/dx\n",
    "x.grad\n",
    "#只有x是variable所以有grad方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(3)\n",
    "x = Variable(x, requires_grad = True)\n",
    "y = x * 2\n",
    "while y.data.norm() < 1000:\n",
    "    y = y * 2\n",
    "gradients = torch.FloatTensor([0.1, 1.0, 0.0001])\n",
    "y.backward(gradients)\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 305.6327819824219)\n",
      "(1, 296.2607116699219)\n",
      "(2, 570.933837890625)\n",
      "(3, 882.069091796875)\n",
      "(4, 886.6889038085938)\n",
      "(5, 1013.9261474609375)\n",
      "(6, 848.6907958984375)\n",
      "(7, 655.14013671875)\n",
      "(8, 449.9483642578125)\n",
      "(9, 125.31603240966797)\n",
      "(10, 264.0281677246094)\n",
      "(11, 357.7687683105469)\n",
      "(12, 155.2150115966797)\n",
      "(13, 159.3505859375)\n",
      "(14, 138.4070281982422)\n",
      "(15, 283.1252136230469)\n",
      "(16, 241.81387329101562)\n",
      "(17, 189.71304321289062)\n",
      "(18, 220.26724243164062)\n",
      "(19, 182.927734375)\n",
      "(20, 176.0771942138672)\n",
      "(21, 120.96100616455078)\n",
      "(22, 83.7100830078125)\n",
      "(23, 82.14942932128906)\n",
      "(24, 45.70379638671875)\n",
      "(25, 40.27718734741211)\n",
      "(26, 49.20444107055664)\n",
      "(27, 58.009647369384766)\n",
      "(28, 32.03042984008789)\n",
      "(29, 36.469242095947266)\n",
      "(30, 41.17305374145508)\n",
      "(31, 36.19930648803711)\n",
      "(32, 38.100650787353516)\n",
      "(33, 32.48198318481445)\n",
      "(34, 33.0024528503418)\n",
      "(35, 19.986068725585938)\n",
      "(36, 20.881635665893555)\n",
      "(37, 12.609193801879883)\n",
      "(38, 11.765188217163086)\n",
      "(39, 12.452439308166504)\n",
      "(40, 9.92991828918457)\n",
      "(41, 7.642862319946289)\n",
      "(42, 5.000237464904785)\n",
      "(43, 8.104769706726074)\n",
      "(44, 7.981945037841797)\n",
      "(45, 5.384096622467041)\n",
      "(46, 4.024317264556885)\n",
      "(47, 3.318084716796875)\n",
      "(48, 4.035863399505615)\n",
      "(49, 5.230893135070801)\n",
      "(50, 5.274465084075928)\n",
      "(51, 4.347218990325928)\n",
      "(52, 3.0260934829711914)\n",
      "(53, 4.536626815795898)\n",
      "(54, 4.584087371826172)\n",
      "(55, 4.446158409118652)\n",
      "(56, 4.967342376708984)\n",
      "(57, 4.957921981811523)\n",
      "(58, 5.066119194030762)\n",
      "(59, 4.563269138336182)\n",
      "(60, 4.604392051696777)\n",
      "(61, 5.389620780944824)\n",
      "(62, 4.335098743438721)\n",
      "(63, 4.771883010864258)\n",
      "(64, 4.781698703765869)\n",
      "(65, 4.624936580657959)\n",
      "(66, 4.503505706787109)\n",
      "(67, 3.9518210887908936)\n",
      "(68, 3.206192970275879)\n",
      "(69, 2.9305880069732666)\n",
      "(70, 3.011312484741211)\n",
      "(71, 3.1476216316223145)\n",
      "(72, 2.9563100337982178)\n",
      "(73, 2.7255520820617676)\n",
      "(74, 2.4371039867401123)\n",
      "(75, 2.4519872665405273)\n",
      "(76, 2.325086832046509)\n",
      "(77, 2.1791679859161377)\n",
      "(78, 2.202346086502075)\n",
      "(79, 2.2290403842926025)\n",
      "(80, 2.262523889541626)\n",
      "(81, 2.250140428543091)\n",
      "(82, 2.1639890670776367)\n",
      "(83, 2.149043321609497)\n",
      "(84, 2.148292064666748)\n",
      "(85, 2.116938352584839)\n",
      "(86, 2.103153944015503)\n",
      "(87, 2.1136131286621094)\n",
      "(88, 2.128514289855957)\n",
      "(89, 2.134483814239502)\n",
      "(90, 2.1248350143432617)\n",
      "(91, 2.1062543392181396)\n",
      "(92, 2.0962798595428467)\n",
      "(93, 2.0973429679870605)\n",
      "(94, 2.0951950550079346)\n",
      "(95, 2.0899674892425537)\n",
      "(96, 2.0859408378601074)\n",
      "(97, 2.0845255851745605)\n",
      "(98, 2.0886809825897217)\n",
      "(99, 2.092318534851074)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1191, 1191, 1) # 1 input image channel, 6 output channels, 5x5 square convolution kernel\n",
    "        self.conv2 = nn.Conv1d(1191, 1191, 1)\n",
    "        self.fc1   = nn.Linear(1191, 800) # an affine operation: y = Wx + b\n",
    "        self.fc2   = nn.Linear(800, 400)\n",
    "        self.fc3   = nn.Linear(400, 10)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x)) # Max pooling over a (2, 2) window z 特征减一半\n",
    "        x = F.relu(self.conv2(x)) # If the size is a square you can only specify a single number\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:] # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "x = Variable(c)\n",
    "y = Variable(b, requires_grad=False)\n",
    "\n",
    "# Construct our model by instantiating the class defined above\n",
    "model = Net()\n",
    "\n",
    "# Construct our loss function and an Optimizer. Training this strange model with\n",
    "# vanilla stochastic gradient descent is tough, so we use momentum\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5, momentum=0.9)\n",
    "for t in range(100):\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y)\n",
    "    print(t, loss.data[0])\n",
    "    all_losses.append(loss.data[0])\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "topk_n, topk_i = model(x).data.topk(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.291208791209\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in xrange(Y_test.size):\n",
    "    if topk_i[i][0] == Y_test[i]:\n",
    "        count+=1\n",
    "\n",
    "print(float(count)/float(Y_test.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = a.view(1092,1191,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "( 0  , 0  ,.,.) = \n",
      "  0.0000e+00  0.0000e+00  0.0000e+00  ...   0.0000e+00  0.0000e+00  0.0000e+00\n",
      "  0.0000e+00  8.5739e+04  3.6875e+01  ...   0.0000e+00  0.0000e+00  1.0000e+00\n",
      "  0.0000e+00  3.2376e+05  2.5887e+01  ...   0.0000e+00  0.0000e+00  1.0000e+00\n",
      "                 ...                   ⋱                   ...                \n",
      "  0.0000e+00  4.3537e+05  3.5166e+01  ...   0.0000e+00  0.0000e+00  1.0000e+00\n",
      "  0.0000e+00  4.8975e+05  3.5734e+01  ...   0.0000e+00  0.0000e+00  1.0000e+00\n",
      "  0.0000e+00  2.9089e+05  4.1883e+01  ...   0.0000e+00  1.0000e+00  0.0000e+00\n",
      "[torch.FloatTensor of size 1x1x7276x1192]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "m = nn.ConstantPad2d((1,0,1,0), 0)\n",
    "input = m(input)\n",
    "print(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "( 0  ,.,.) = \n",
       "  3.6679e+05\n",
       "  5.2216e+01\n",
       " -1.7245e+02\n",
       "     ⋮      \n",
       "  0.0000e+00\n",
       "  1.0000e+00\n",
       "  0.0000e+00\n",
       "\n",
       "( 1  ,.,.) = \n",
       "  2.4482e+05\n",
       "  4.8178e+01\n",
       " -1.2537e+02\n",
       "     ⋮      \n",
       "  0.0000e+00\n",
       "  0.0000e+00\n",
       "  1.0000e+00\n",
       "\n",
       "( 2  ,.,.) = \n",
       "  1.1491e+05\n",
       "  3.3355e+01\n",
       " -1.1902e+02\n",
       "     ⋮      \n",
       "  0.0000e+00\n",
       "  1.0000e+00\n",
       "  0.0000e+00\n",
       " ... \n",
       "\n",
       "(1089,.,.) = \n",
       "  2.0863e+05\n",
       "  3.3971e+01\n",
       " -1.2113e+02\n",
       "     ⋮      \n",
       "  0.0000e+00\n",
       "  0.0000e+00\n",
       "  1.0000e+00\n",
       "\n",
       "(1090,.,.) = \n",
       "  4.1969e+04\n",
       "  3.3919e+01\n",
       " -1.1947e+02\n",
       "     ⋮      \n",
       "  0.0000e+00\n",
       "  0.0000e+00\n",
       "  1.0000e+00\n",
       "\n",
       "(1091,.,.) = \n",
       "  1.1450e+05\n",
       "  4.1919e+01\n",
       " -1.2485e+02\n",
       "     ⋮      \n",
       "  0.0000e+00\n",
       "  1.0000e+00\n",
       "  0.0000e+00\n",
       "[torch.FloatTensor of size 1092x1191x1]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = Variable(c)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "(0 ,.,.) = \n",
       "\n",
       "Columns 0 to 8 \n",
       "  -0.1936 -0.6304 -0.8411 -1.2612 -0.0812  0.8946  0.0219 -0.1573 -0.6512\n",
       " -0.4543 -1.6682  0.2163 -0.7512 -1.5829  0.2780 -0.1479 -0.9394  0.0997\n",
       "  1.4476 -0.1878 -0.1277  0.4280 -0.2548 -0.5982  0.0191 -0.6092 -0.2152\n",
       "  0.9723  0.0906  0.0756  0.4512  0.4943  0.0362 -0.3273  0.1057 -0.0610\n",
       "  1.1797  0.0102 -0.0353 -0.6266  0.9450  0.2118 -0.5839 -0.1127  0.4460\n",
       "  0.7509 -0.0224  0.2855 -0.0995  0.5524 -0.3182  0.3746  0.4912  0.0287\n",
       "  0.2101 -0.1660 -0.3724  0.6290  0.4723 -0.0508  0.5499  0.5503 -0.4621\n",
       " -0.9565  0.2953  0.1989  0.0417  0.6309  0.2004  1.1365  0.9270  0.0103\n",
       " -1.0803  0.8069  0.2605  0.0291  0.3395  0.1008 -0.6125  0.6509 -0.5378\n",
       "  0.2490 -1.7430  0.0091  0.4654  0.2582 -0.4039 -0.5528  0.5561  1.1241\n",
       " -0.1867 -0.2212  0.9999  0.3781  0.1579  0.1973 -0.1380  0.4860  0.4885\n",
       " -0.4869 -0.5138 -0.6821  0.4750  0.3146  0.5517 -0.1379 -0.1481 -0.3867\n",
       " -0.2550  1.0557  0.3911 -0.0236  1.0585  0.7120  0.0993  0.5286 -0.3901\n",
       " -0.0066  0.3046  0.0489  0.5004  0.2549  0.4473  0.5849  0.7409  0.4107\n",
       "  0.5787 -0.9482 -0.4572 -0.0773  0.9533 -0.3140  0.1546 -0.0345 -0.2437\n",
       " -0.3377 -1.1839 -0.1090 -0.1300  0.3341 -0.0797  0.2556  0.5494 -0.2277\n",
       " -0.6507  0.2222  0.0104  0.5789 -0.3697  0.0699  0.9707  0.4864  0.3479\n",
       " -0.3851  0.0818 -1.0216  0.5612 -0.1575  0.4101  0.2636  0.7925  0.5677\n",
       "  0.4122 -0.3062 -0.1020 -0.5742 -0.8423 -0.5955 -0.7021  0.0574  0.9096\n",
       " -0.4951 -0.4415  0.3816 -2.2780 -0.2878  0.0795 -0.1510 -1.4665 -0.0876\n",
       "  0.2568  0.7329 -0.2845 -0.6058 -0.8491 -0.8119 -0.4590  0.7480  1.1152\n",
       " -0.1763 -0.9482  0.2753 -0.8136  0.3354  0.2772 -0.2181 -0.0735  0.3938\n",
       " -0.1292 -0.5919 -0.1168 -0.0882 -0.9190 -0.7614  0.3929  0.5494  0.1495\n",
       " -0.4926  1.0252 -0.3238  0.1447 -0.8515  0.5798 -0.7009  0.2353  0.4151\n",
       " -1.3203 -0.4961  0.1326  0.0500 -0.2217  0.0694  0.4138  0.2765  0.9215\n",
       "  0.6710 -1.0822 -0.1074 -0.8099  0.6582  0.1617 -0.5418  0.3188  1.0777\n",
       "  0.1738 -0.7503 -0.6242  0.4926  0.3952  1.0640 -0.6863 -0.6105 -0.4018\n",
       "  0.4264 -0.1958 -0.1398  0.3772  0.8648 -0.5956 -0.3415 -0.2725  0.3885\n",
       " -0.6863  1.3607  0.4227  0.6410 -0.3826 -0.0927  0.7251  0.2372 -0.8783\n",
       "  0.6679  0.6340  0.3080 -0.0223 -0.2651 -0.2857  0.5205 -1.8843 -0.3018\n",
       " -0.0630  0.1951  0.2735  0.0456 -0.4407  0.9347  0.0326 -0.8608  0.2198\n",
       " -0.7333 -0.1834 -1.3683 -1.0777  0.2199  1.3403 -0.2309  0.6983 -1.0042\n",
       " -0.0717  0.5392 -0.7810  0.0344 -0.1255  1.2253  0.3005 -0.0889 -0.7955\n",
       "\n",
       "Columns 9 to 15 \n",
       "  -0.5680 -0.8600  0.1960  0.2665  0.1194 -0.3089  0.3377\n",
       "  0.6571 -0.1988 -1.3315 -0.9264  1.3033 -0.4343 -0.0886\n",
       "  0.2697  0.3681  0.5877 -0.0272 -0.8684  0.2612 -0.2470\n",
       " -0.0019  0.7231  1.0225 -0.7404  0.3227  0.1851  0.2700\n",
       "  0.1075 -0.2952  0.0685  0.1341  0.2429 -0.1895  0.8606\n",
       "  0.1363 -1.2013  0.8138  1.2478  0.3895  0.8896 -0.3703\n",
       "  0.5643  0.5718  0.4624 -0.4290  0.4379 -0.9442  0.1433\n",
       " -0.7326 -0.7910  0.1492  0.1053 -0.2606  0.3812 -0.0791\n",
       " -0.2403 -0.2997 -0.3492 -0.0634 -0.5011 -0.3691  0.3834\n",
       "  0.5997 -1.4669 -0.4240  0.0369  1.6469 -1.0500  0.3455\n",
       "  0.2493  0.3101  0.3781  0.2216 -0.1154 -0.5261  0.3120\n",
       "  1.0603 -0.6998 -1.2060 -0.4545  0.0449 -0.2487 -1.2852\n",
       " -0.2018 -0.1915  0.6022  0.0727 -0.5064 -0.0064  0.4890\n",
       " -1.0540 -0.1029  0.1860  0.6787  0.3375 -0.3590  0.3257\n",
       " -0.3429 -0.9160 -0.2427 -0.5362 -0.0708 -0.4914 -1.0002\n",
       "  0.3685  0.2775  0.6208  1.0101  0.0839  0.8488 -0.1978\n",
       " -0.1328  0.6510  0.0052 -0.5661 -0.1392 -0.4947 -0.4950\n",
       " -0.1649 -0.0951 -0.0197  0.1825 -0.1059 -0.1150  0.1594\n",
       "  0.2088  0.3224  0.2681 -0.1360 -0.4336 -1.3762 -0.3443\n",
       " -0.9239  0.2186 -0.3863  0.0192  0.0206  1.0300  0.5220\n",
       " -0.0008  0.1611  0.0229  1.2040  0.0382 -0.4632  1.5420\n",
       " -1.0008  0.5973 -0.0608  0.4909  0.3776 -0.2195  1.1423\n",
       " -0.9103  0.6637 -0.0993  0.6597  0.6893 -0.2027 -0.0355\n",
       " -0.0180  0.3069  0.0439  0.9407 -0.4350  0.2083  0.4809\n",
       " -0.3443  0.7205 -0.7857  0.5815  0.1667  1.3525 -0.4348\n",
       "  0.6360 -0.2864 -0.8256  0.1357 -0.3832  0.5488 -0.1549\n",
       "  0.4643 -1.1683  0.1169 -0.4014  0.4188 -1.0408 -0.2639\n",
       "  0.0034 -1.2927 -0.4413  0.1074 -0.2925 -0.2740  0.2265\n",
       " -0.6689  0.4100  1.0737 -0.6987 -0.5297  0.0332 -0.0177\n",
       "  0.7565 -0.5498 -1.1766 -0.1567  0.2849  0.0063 -1.1609\n",
       "  1.3166 -0.0674 -1.2406  0.4404 -0.1384 -0.2900 -0.7105\n",
       " -0.8267  0.8733  0.6042 -0.2763  0.0945 -0.5302 -0.3335\n",
       "  0.8822  0.4327  0.2672 -0.0324 -0.8430  0.6087  0.5117\n",
       "[torch.FloatTensor of size 1x33x16]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = nn.Conv1d(16, 33, 3, stride=3)\n",
    "input = Variable(torch.randn(1, 16, 50))\n",
    "output = m(input)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>>> total trainning batch number: 600\n",
      "==>>> total testing batch number: 100\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "## load mnist dataset\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "root = './data'\n",
    "download = False  # download MNIST dataset or not\n",
    "\n",
    "trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])\n",
    "train_set = dset.MNIST(root=root, train=True, transform=trans, download=download)\n",
    "test_set = dset.MNIST(root=root, train=False, transform=trans)\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "                 dataset=train_set,\n",
    "                 batch_size=batch_size,\n",
    "                 shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "                dataset=test_set,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=False)\n",
    "\n",
    "print '==>>> total trainning batch number: {}'.format(len(train_loader))\n",
    "print '==>>> total testing batch number: {}'.format(len(test_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>>> epoch: 0, batch index: 600, train loss: 2.309628\n",
      "==>>> epoch: 1, batch index: 600, train loss: 2.306904\n",
      "==>>> epoch: 2, batch index: 600, train loss: 2.309014\n",
      "==>>> epoch: 3, batch index: 600, train loss: 2.311563\n",
      "==>>> epoch: 4, batch index: 600, train loss: 2.306196\n",
      "==>>> epoch: 5, batch index: 600, train loss: 2.309915\n",
      "==>>> epoch: 6, batch index: 600, train loss: 2.308569\n",
      "==>>> epoch: 7, batch index: 600, train loss: 2.312373\n",
      "==>>> epoch: 8, batch index: 600, train loss: 2.309463\n",
      "==>>> epoch: 9, batch index: 600, train loss: 2.309551\n"
     ]
    }
   ],
   "source": [
    "class MLPNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 500)\n",
    "        self.fc2 = nn.Linear(500, 256)\n",
    "        self.fc3 = nn.Linear(256,10)\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "all_losses = []\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "criteria = nn.CrossEntropyLoss()\n",
    "model = MLPNet()\n",
    "for epoch in range(10):\n",
    "    ave_loss = 0\n",
    "    for batch_idx, (x,y) in enumerate(train_loader):\n",
    "        x, y = Variable(x), Variable(y)\n",
    "        # Forward pass: Compute predicted y by passing x to the model\n",
    "        y_pred = model(x)\n",
    "        # Compute and print loss\n",
    "        loss = criterion(y_pred, y)\n",
    "        ave_loss = ave_loss * 0.9 + loss.data[0] * 0.1\n",
    "        # Zero gradients, perform a backward pass, and update the weights.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if (batch_idx+1) % 100 == 0 or (batch_idx+1) == len(train_loader):\n",
    "            print '==>>> epoch: {}, batch index: {}, train loss: {:.6f}'.format(\n",
    "                epoch, batch_idx+1, ave_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ellise/anaconda/lib/python2.7/site-packages/ipykernel_launcher.py:1: UserWarning: torch.range is deprecated in favor of torch.arange and will be removed in 0.3. Note that arange generates values in [start; end), not [start; end].\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "a = torch.range(1, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, (x,y) in enumerate(test_loader):\n",
    "        x, y = Variable(x), Variable(y)\n",
    "        y_pred = model(x)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
